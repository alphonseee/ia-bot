# ğŸ”¥ Hephaestus

**Assistant IA Coach Musculation** â€” Projet Epitech 4Ã¨me annÃ©e


---

## ğŸ“‹ Sommaire

- [PrÃ©sentation](#-prÃ©sentation)
- [Architecture](#-architecture)
- [Stack technique](#-stack-technique)
- [PrÃ©requis](#-prÃ©requis)
- [Installation](#-installation)
- [Lancement](#-lancement)
- [Ingestion des donnÃ©es](#-ingestion-des-donnÃ©es)
- [Structure du projet](#-structure-du-projet)
- [API Reference](#-api-reference)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Ã‰quipe](#-Ã©quipe)

---

## ğŸ¯ PrÃ©sentation

**Hephaestus** est un assistant IA spÃ©cialisÃ© dans la musculation et l'entraÃ®nement de force.

### Le problÃ¨me

- L'information sur la musculation est fragmentÃ©e et souvent peu fiable
- Les coachs coÃ»tent cher
- Les IA gÃ©nÃ©riques (ChatGPT, etc.) hallucinent sur les sujets sportifs

### Notre solution

- Une IA spÃ©cialisÃ©e **uniquement** sur la musculation
- Une base de connaissances vÃ©rifiÃ©e (sites francophones de rÃ©fÃ©rence)
- Des rÃ©ponses **sourcÃ©es** avec citations
- 100% local (Ollama), aucune donnÃ©e envoyÃ©e Ã  des serveurs externes

### Comment Ã§a marche ?

On utilise la technique **RAG** (Retrieval-Augmented Generation) :

1. On scrape des sites de musculation francophones
2. On dÃ©coupe le contenu en chunks et on gÃ©nÃ¨re des embeddings (vecteurs)
3. Quand l'utilisateur pose une question, on cherche les chunks les plus pertinents
4. On injecte ce contexte dans le prompt du LLM
5. Le LLM gÃ©nÃ¨re une rÃ©ponse basÃ©e sur des sources rÃ©elles

---

## ğŸ“ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js App   â”‚â”€â”€â”€â”€â–¶â”‚  FastAPI MCP Server  â”‚â”€â”€â”€â”€â–¶â”‚   Ollama    â”‚
â”‚   (Frontend)    â”‚â—€â”€â”€â”€â”€â”‚  (Backend + RAG)     â”‚â—€â”€â”€â”€â”€â”‚  (Mistral)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       SSE                         â”‚
    streaming                      â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Supabase PostgreSQL â”‚
                        â”‚  + pgvector          â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de donnÃ©es

**Phase 1 â€” Ingestion (one-shot)**

```
Sites web â”€â”€â–¶ Playwright (crawl) â”€â”€â–¶ BeautifulSoup (extract)
                                              â”‚
                                              â–¼
Supabase â—€â”€â”€ Ollama (embed) â—€â”€â”€ tiktoken (chunk ~1000 tokens)
```

**Phase 2 â€” Chat (runtime)**

```
Question â”€â”€â–¶ Embed â”€â”€â–¶ Recherche vectorielle â”€â”€â–¶ Top 5 chunks
                                                      â”‚
                                                      â–¼
RÃ©ponse â—€â”€â”€ Ollama (Mistral) â—€â”€â”€ Prompt + contexte RAG
```

---

## ğŸ§° Stack technique

| Composant | Technologies |
|-----------|--------------|
| **Frontend** | Next.js 14 (App Router), React 18, TypeScript, Tailwind CSS |
| **Backend** | FastAPI, Python 3.11+, Pydantic |
| **LLM** | Ollama â€” Mistral (chat), nomic-embed-text (embeddings) |
| **Base de donnÃ©es** | Supabase PostgreSQL + pgvector (HNSW index) |
| **Scraping** | Playwright (headless browser), BeautifulSoup4 |
| **Protocole** | JSON-RPC over HTTP, SSE streaming |
| **Monorepo** | pnpm workspaces |

---

## ğŸ›  PrÃ©requis

| Outil | Version | Installation |
|-------|---------|--------------|
| Node.js | 20+ LTS | [nodejs.org](https://nodejs.org) |
| Python | 3.11+ | [python.org](https://python.org) |
| pnpm | 8+ | `npm install -g pnpm` |
| Ollama | latest | [ollama.ai](https://ollama.ai) |
| Supabase | â€” | Compte gratuit sur [supabase.com](https://supabase.com) |

### ModÃ¨les Ollama requis

```bash
ollama pull mistral
ollama pull nomic-embed-text
```

---

## ğŸš€ Installation

### 1. Clone le repo

```bash
git clone https://github.com/alphonseee/hephaestus.git
cd ia-bot
```

### 2. Configuration environnement

```bash
cp .env.example .env
```

Ã‰dite `.env` avec tes credentials Supabase :

```env
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6...
```

### 3. Setup la base de donnÃ©es

Dans **Supabase Dashboard â†’ SQL Editor**, exÃ©cute ce script :

```sql
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url TEXT NOT NULL UNIQUE,
    domain TEXT NOT NULL,
    title TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source_id UUID REFERENCES sources(id) ON DELETE CASCADE,
    url TEXT NOT NULL,
    title TEXT,
    content_text TEXT,
    content_hash TEXT NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    chunk_text TEXT NOT NULL,
    token_count INTEGER,
    embedding VECTOR(768),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX chunks_embedding_idx ON chunks 
USING hnsw (embedding vector_cosine_ops);

CREATE INDEX documents_hash_idx ON documents 
USING hash (content_hash);

CREATE OR REPLACE FUNCTION match_chunks(
    query_embedding VECTOR(768),
    match_count INT DEFAULT 5,
    min_similarity FLOAT DEFAULT 0.5
)
RETURNS TABLE (
    chunk_id UUID,
    chunk_text TEXT,
    document_url TEXT,
    document_title TEXT,
    similarity FLOAT
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT
        c.id AS chunk_id,
        c.chunk_text,
        d.url AS document_url,
        d.title AS document_title,
        1 - (c.embedding <=> query_embedding) AS similarity
    FROM chunks c
    JOIN documents d ON c.document_id = d.id
    WHERE 1 - (c.embedding <=> query_embedding) >= min_similarity
    ORDER BY c.embedding <=> query_embedding
    LIMIT match_count;
END;
$$;
```

### 4. Setup le backend

```powershell
cd services/mcp-server
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
```

### 5. Setup le script d'ingestion

```powershell
cd scripts/ingest
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
playwright install chromium
```

### 6. Setup le frontend

```bash
cd apps/web
pnpm install
```

---

## â–¶ï¸ Lancement

**Terminal 1 â€” Backend**

```powershell
cd services/mcp-server
.venv\Scripts\activate
uvicorn src.main:app --reload --port 8000
```

**Terminal 2 â€” Frontend**

```bash
cd apps/web
pnpm dev
```

**AccÃ¨s** : http://localhost:3000

---

## ğŸ“¥ Ingestion des donnÃ©es

Avant d'utiliser l'IA, il faut remplir la knowledge base.

### 1. Configure les sources

Ã‰dite `scripts/ingest/src/config.py` :

```python
SEED_URLS = [
    "https://www.superphysique.org/articles/",
    "https://www.espace-musculation.com/",
    "https://www.musculaction.com/",
]
```

### 2. Lance le scraping

```powershell
cd scripts/ingest
.venv\Scripts\activate
python -m src.main
```

### ParamÃ¨tres configurables

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `MAX_PAGES_PER_DOMAIN` | 50 | Nombre max de pages par site |
| `MAX_DEPTH` | 2 | Profondeur de crawl |
| `REQUEST_DELAY_SECONDS` | 1.5 | DÃ©lai entre requÃªtes (respectueux) |
| `CHUNK_SIZE_TOKENS` | 1000 | Taille des chunks |
| `CHUNK_OVERLAP_TOKENS` | 150 | Overlap entre chunks |

---

## ğŸ“ Structure du projet

```
hephaestus/
â”œâ”€â”€ .env                          # Variables d'environnement (centralisÃ©)
â”œâ”€â”€ .env.example                  # Template
â”œâ”€â”€ package.json                  # Config pnpm workspace
â”œâ”€â”€ pnpm-workspace.yaml
â”‚
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ web/                      # Frontend Next.js
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ app/              # Pages (App Router)
â”‚       â”‚   â”‚   â”œâ”€â”€ page.tsx      # Landing page
â”‚       â”‚   â”‚   â””â”€â”€ chat/         # Interface chat
â”‚       â”‚   â”œâ”€â”€ components/       # Composants React
â”‚       â”‚   â””â”€â”€ lib/              # Client MCP, utils
â”‚       â”œâ”€â”€ package.json
â”‚       â””â”€â”€ tailwind.config.js
â”‚
â”œâ”€â”€ services/
â”‚   â””â”€â”€ mcp-server/               # Backend FastAPI
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ main.py           # Entry point
â”‚       â”‚   â”œâ”€â”€ config.py         # Settings Pydantic
â”‚       â”‚   â”œâ”€â”€ models.py         # Schemas
â”‚       â”‚   â”œâ”€â”€ mcp/
â”‚       â”‚   â”‚   â”œâ”€â”€ router.py     # Endpoints JSON-RPC + SSE
â”‚       â”‚   â”‚   â””â”€â”€ tools.py      # Tools MCP
â”‚       â”‚   â”œâ”€â”€ chat/
â”‚       â”‚   â”‚   â”œâ”€â”€ service.py    # Logique RAG
â”‚       â”‚   â”‚   â”œâ”€â”€ sessions.py   # MÃ©moire de session (TTL)
â”‚       â”‚   â”‚   â””â”€â”€ prompts.py    # System prompt
â”‚       â”‚   â”œâ”€â”€ kb/
â”‚       â”‚   â”‚   â””â”€â”€ search.py     # Recherche vectorielle
â”‚       â”‚   â””â”€â”€ ollama/
â”‚       â”‚       â””â”€â”€ client.py     # Client Ollama
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest/                   # Pipeline d'ingestion
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ main.py           # Orchestrateur
â”‚       â”‚   â”œâ”€â”€ config.py         # Config + SEED_URLS
â”‚       â”‚   â”œâ”€â”€ crawler.py        # Playwright
â”‚       â”‚   â”œâ”€â”€ extractor.py      # BeautifulSoup
â”‚       â”‚   â”œâ”€â”€ chunker.py        # tiktoken
â”‚       â”‚   â”œâ”€â”€ embedder.py       # Ollama embeddings
â”‚       â”‚   â”œâ”€â”€ robots.py         # Respect robots.txt
â”‚       â”‚   â””â”€â”€ db.py             # Client Supabase
â”‚       â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ supabase/
    â””â”€â”€ migrations/               # Scripts SQL
```

---

## ğŸ”Œ API Reference

### Health Check

```bash
curl http://localhost:8000/health
```

### Lister les tools MCP

```bash
curl -X POST http://localhost:8000/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","id":"1","method":"tools/list","params":{}}'
```

### Recherche dans la KB

```bash
curl -X POST http://localhost:8000/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc":"2.0",
    "id":"2",
    "method":"tools/call",
    "params":{
      "name":"search_knowledge_base",
      "arguments":{"query":"squat technique","k":5}
    }
  }'
```

### Chat (sans streaming)

```bash
curl -X POST http://localhost:8000/mcp \
  -H "Content-Type: application/json" \
  -d '{
    "jsonrpc":"2.0",
    "id":"3",
    "method":"chat",
    "params":{
      "session_id":"test-123",
      "message":"Comment amÃ©liorer mon dÃ©veloppÃ© couchÃ© ?",
      "stream":false
    }
  }'
```

### Chat (SSE streaming)

```bash
curl "http://localhost:8000/mcp/stream?session_id=test-123&message=Exercices%20dos"
```

---

## âœ¨ FonctionnalitÃ©s

| Feature | Description |
|---------|-------------|
| **RAG** | RÃ©ponses basÃ©es sur une knowledge base, avec sources citÃ©es |
| **Streaming SSE** | Affichage token par token en temps rÃ©el |
| **MÃ©moire de session** | Historique de conversation cÃ´tÃ© serveur (TTL 1h) |
| **Filtrage thÃ©matique** | Refuse les questions hors-sujet (politique, mÃ©dical...) |
| **Anti-hallucination** | Prompt strict pour citer uniquement les sources KB |
| **100% local** | Tout tourne en local avec Ollama, pas d'API externe |
| **Respect robots.txt** | Le crawler respecte les rÃ¨gles des sites |

---

## ğŸ“„ Licence

MIT â€” Projet Ã©ducatif Epitech
